# Understanding Unicode

## unicode 1

1. `chr(0)`  returns '\x00'
2. String representation of '\x00' is "'\\x00'", while its printed representation appears as an invisible, non-printing character.
3. 'this is a test\x00string' and this is a teststring

## unicode 2

1. The sequence of UTF-16 or UTF-32 code is longer than UTF-8
2. UTF-8 use 1-4 bytes to represent a character
3. `\xb0\x00` is a two-byte sequence that does not decode to any Unicode character in UTF-8, because the second byte must be in the range 0x80–0xBF (start with binary 10), but `\x00` does not satisfy this condition.

# BPE

1. Vocabulary Initialization
2. Pre-tokenization: ( 1. pre-tokenization can save time, avoid go over the corpus each time we merge. 2. avoid token generated by auto merging like `dog!` `dog.` , even though they have high semantic similarity

   1. 

      ```py
      >>> import regex as re
      >>> PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
      >>> re.findall(PAT, "some text that i'll pre-tokenize")
      ['some', ' text', ' that', ' i', "'ll", ' pre', '-', 'tokenize']
      ```


3. Compute BPE merges

   1. Now the input text is converted to pre-tokens which represented by a sequence of UTF-8 bytes.
   2. preferring the lexicographically greater pair (if the pairs (“A”, “B”), (“A”, “C”), (“B”, “ZZ”),and (“BA”, “A”) all have the highest frequency, we’d merge (“BA”, “A”)

      ```py
      >>> max([("A", "B"), ("A", "C"), ("B", "ZZ"), ("BA", "A")])
      ('BA', 'A')
      ```


4. Special tokens

   1. Tokens like `<|endoftext|>` representing boundaries between documents should be treated as a single token and will never be merged.

5. 

